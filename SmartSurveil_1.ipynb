{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b53ba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ac0d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qai-hub configuration saved to C:\\Users\\qc_de/.qai_hub/client.ini\n",
      "==================== C:\\Users\\qc_de/.qai_hub/client.ini ====================\n",
      "[api]\n",
      "api_token = atbrq5wg15d5xbvszpmhw8k17kaldjq58p5luzou\n",
      "api_url = https://app.aihub.qualcomm.com\n",
      "web_url = https://app.aihub.qualcomm.com\n",
      "verbose = True\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 01:52:34.078 - INFO - Enabling verbose logging.\n",
      "C:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\qai_hub\\_cli.py:384: UserWarning: Overwriting configuration: C:\\Users\\qc_de/.qai_hub/client.ini (previous configuration saved to C:\\Users\\qc_de/.qai_hub/client.ini.bak)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!qai-hub configure --api_token atbrq5wg15d5xbvszpmhw8k17kaldjq58p5luzou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0aa6205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+--------------+----------+---------+---------------------------------------------------+---------------------------------------------------------+\n",
      "|             Device            |      OS      |  Vendor  |   Type  |                      Chipset                      |                      CLI Invocation                     |\n",
      "+-------------------------------+--------------+----------+---------+---------------------------------------------------+---------------------------------------------------------+\n",
      "|    Google Pixel 3 (Family)    |  Android 10  |  Google  |  Phone  |          qualcomm-snapdragon-845, sdm845          |    --device \"Google Pixel 3 (Family)\" --device-os 10    |\n",
      "|         Google Pixel 3        |  Android 10  |  Google  |  Phone  |          qualcomm-snapdragon-845, sdm845          |         --device \"Google Pixel 3\" --device-os 10        |\n",
      "|        Google Pixel 3a        |  Android 10  |  Google  |  Phone  |          qualcomm-snapdragon-670, sdm670          |        --device \"Google Pixel 3a\" --device-os 10        |\n",
      "|       Google Pixel 3 XL       |  Android 10  |  Google  |  Phone  |          qualcomm-snapdragon-845, sdm845          |       --device \"Google Pixel 3 XL\" --device-os 10       |\n",
      "|         Google Pixel 4        |  Android 10  |  Google  |  Phone  |          qualcomm-snapdragon-855, sm8150          |         --device \"Google Pixel 4\" --device-os 10        |\n",
      "|         Google Pixel 4        |  Android 11  |  Google  |  Phone  |          qualcomm-snapdragon-855, sm8150          |         --device \"Google Pixel 4\" --device-os 11        |\n",
      "|        Google Pixel 4a        |  Android 11  |  Google  |  Phone  |        qualcomm-snapdragon-730g, sm7150-ab        |        --device \"Google Pixel 4a\" --device-os 11        |\n",
      "|         Google Pixel 5        |  Android 11  |  Google  |  Phone  |          qualcomm-snapdragon-765g, sm7250         |         --device \"Google Pixel 5\" --device-os 11        |\n",
      "|     Samsung Galaxy Tab S7     |  Android 11  | Samsung  |  Tablet |        qualcomm-snapdragon-865+, sm8250-ab        |     --device \"Samsung Galaxy Tab S7\" --device-os 11     |\n",
      "|  Samsung Galaxy Tab A8 (2021) |  Android 11  | Samsung  |  Tablet |          qualcomm-snapdragon-429, sdm429          |  --device \"Samsung Galaxy Tab A8 (2021)\" --device-os 11 |\n",
      "| Samsung Galaxy Note 20 (Intl) |  Android 11  | Samsung  |  Phone  |                 samsung-exynos-990                | --device \"Samsung Galaxy Note 20 (Intl)\" --device-os 11 |\n",
      "|  Samsung Galaxy S21 (Family)  |  Android 11  | Samsung  |  Phone  |          qualcomm-snapdragon-888, sm8350          |  --device \"Samsung Galaxy S21 (Family)\" --device-os 11  |\n",
      "|       Samsung Galaxy S21      |  Android 11  | Samsung  |  Phone  |          qualcomm-snapdragon-888, sm8350          |       --device \"Samsung Galaxy S21\" --device-os 11      |\n",
      "|      Samsung Galaxy S21+      |  Android 11  | Samsung  |  Phone  |          qualcomm-snapdragon-888, sm8350          |      --device \"Samsung Galaxy S21+\" --device-os 11      |\n",
      "|    Samsung Galaxy S21 Ultra   |  Android 11  | Samsung  |  Phone  |          qualcomm-snapdragon-888, sm8350          |    --device \"Samsung Galaxy S21 Ultra\" --device-os 11   |\n",
      "|    Xiaomi Redmi Note 10 5G    |  Android 11  | Oneplus  |  Phone  |         qualcomm-snapdragon-678, sm6150-ac        |    --device \"Xiaomi Redmi Note 10 5G\" --device-os 11    |\n",
      "|       Google Pixel 3a XL      |  Android 12  |  Google  |  Phone  |          qualcomm-snapdragon-670, sdm670          |       --device \"Google Pixel 3a XL\" --device-os 12      |\n",
      "|        Google Pixel 4a        |  Android 12  |  Google  |  Phone  |        qualcomm-snapdragon-730g, sm7150-ab        |        --device \"Google Pixel 4a\" --device-os 12        |\n",
      "|    Google Pixel 5 (Family)    |  Android 12  |  Google  |  Phone  |          qualcomm-snapdragon-765g, sm7250         |    --device \"Google Pixel 5 (Family)\" --device-os 12    |\n",
      "|         Google Pixel 5        |  Android 12  |  Google  |  Phone  |          qualcomm-snapdragon-765g, sm7250         |         --device \"Google Pixel 5\" --device-os 12        |\n",
      "|       Google Pixel 5a 5G      |  Android 12  |  Google  |  Phone  |          qualcomm-snapdragon-765g, sm7250         |       --device \"Google Pixel 5a 5G\" --device-os 12      |\n",
      "|         Google Pixel 6        |  Android 12  |  Google  |  Phone  |                   google-tensor                   |         --device \"Google Pixel 6\" --device-os 12        |\n",
      "|     Samsung Galaxy A53 5G     |  Android 12  | Samsung  |  Phone  |                samsung-exynos-1280                |     --device \"Samsung Galaxy A53 5G\" --device-os 12     |\n",
      "|     Samsung Galaxy A73 5G     |  Android 12  | Samsung  |  Phone  |          qualcomm-snapdragon-778g, sm7325         |     --device \"Samsung Galaxy A73 5G\" --device-os 12     |\n",
      "|       RB3 Gen 2 (Proxy)       |  Android 12  | Qualcomm |   Iot   |               qualcomm-qcs6490-proxy              |       --device \"RB3 Gen 2 (Proxy)\" --device-os 12       |\n",
      "|        QCS6490 (Proxy)        |  Android 12  | Qualcomm |   Iot   |               qualcomm-qcs6490-proxy              |        --device \"QCS6490 (Proxy)\" --device-os 12        |\n",
      "|          RB5 (Proxy)          |  Android 12  | Qualcomm |   Iot   |               qualcomm-qcs8250-proxy              |          --device \"RB5 (Proxy)\" --device-os 12          |\n",
      "|        QCS8250 (Proxy)        |  Android 12  | Qualcomm |   Iot   |               qualcomm-qcs8250-proxy              |        --device \"QCS8250 (Proxy)\" --device-os 12        |\n",
      "|        QCS8550 (Proxy)        |  Android 12  | Qualcomm |   Iot   |               qualcomm-qcs8550-proxy              |        --device \"QCS8550 (Proxy)\" --device-os 12        |\n",
      "|  Samsung Galaxy S21 (Family)  |  Android 12  | Samsung  |  Phone  |          qualcomm-snapdragon-888, sm8350          |  --device \"Samsung Galaxy S21 (Family)\" --device-os 12  |\n",
      "|       Samsung Galaxy S21      |  Android 12  | Samsung  |  Phone  |          qualcomm-snapdragon-888, sm8350          |       --device \"Samsung Galaxy S21\" --device-os 12      |\n",
      "|    Samsung Galaxy S21 Ultra   |  Android 12  | Samsung  |  Phone  |          qualcomm-snapdragon-888, sm8350          |    --device \"Samsung Galaxy S21 Ultra\" --device-os 12   |\n",
      "|  Samsung Galaxy S22 (Family)  |  Android 12  | Samsung  |  Phone  |         qualcomm-snapdragon-8gen1, sm8450         |  --device \"Samsung Galaxy S22 (Family)\" --device-os 12  |\n",
      "|  Samsung Galaxy S22 Ultra 5G  |  Android 12  | Samsung  |  Phone  |         qualcomm-snapdragon-8gen1, sm8450         |  --device \"Samsung Galaxy S22 Ultra 5G\" --device-os 12  |\n",
      "|     Samsung Galaxy S22 5G     |  Android 12  | Samsung  |  Phone  |         qualcomm-snapdragon-8gen1, sm8450         |     --device \"Samsung Galaxy S22 5G\" --device-os 12     |\n",
      "|     Samsung Galaxy S22+ 5G    |  Android 12  | Samsung  |  Phone  |         qualcomm-snapdragon-8gen1, sm8450         |     --device \"Samsung Galaxy S22+ 5G\" --device-os 12    |\n",
      "|     Samsung Galaxy Tab S8     |  Android 12  | Samsung  |  Tablet |         qualcomm-snapdragon-8gen1, sm8450         |     --device \"Samsung Galaxy Tab S8\" --device-os 12     |\n",
      "|       Xiaomi 12 (Family)      |  Android 12  |  Xiaomi  |  Phone  |         qualcomm-snapdragon-8gen1, sm8450         |       --device \"Xiaomi 12 (Family)\" --device-os 12      |\n",
      "|           Xiaomi 12           |  Android 12  |  Xiaomi  |  Phone  |         qualcomm-snapdragon-8gen1, sm8450         |           --device \"Xiaomi 12\" --device-os 12           |\n",
      "|         Xiaomi 12 Pro         |  Android 12  |  Xiaomi  |  Phone  |         qualcomm-snapdragon-8gen1, sm8450         |         --device \"Xiaomi 12 Pro\" --device-os 12         |\n",
      "|    Google Pixel 6 (Family)    |  Android 13  |  Google  |  Phone  |                   google-tensor                   |    --device \"Google Pixel 6 (Family)\" --device-os 13    |\n",
      "|         Google Pixel 6        |  Android 13  |  Google  |  Phone  |                   google-tensor                   |         --device \"Google Pixel 6\" --device-os 13        |\n",
      "|        Google Pixel 6a        |  Android 13  |  Google  |  Phone  |                   google-tensor                   |        --device \"Google Pixel 6a\" --device-os 13        |\n",
      "|    Google Pixel 7 (Family)    |  Android 13  |  Google  |  Phone  |                  google-tensor-g2                 |    --device \"Google Pixel 7 (Family)\" --device-os 13    |\n",
      "|         Google Pixel 7        |  Android 13  |  Google  |  Phone  |                  google-tensor-g2                 |         --device \"Google Pixel 7\" --device-os 13        |\n",
      "|       Google Pixel 7 Pro      |  Android 13  |  Google  |  Phone  |                  google-tensor-g2                 |       --device \"Google Pixel 7 Pro\" --device-os 13      |\n",
      "|     Samsung Galaxy A14 5G     |  Android 13  | Samsung  |  Phone  |                samsung-exynos-1330                |     --device \"Samsung Galaxy A14 5G\" --device-os 13     |\n",
      "|     Samsung Galaxy S22 5G     |  Android 13  | Samsung  |  Phone  |         qualcomm-snapdragon-8gen1, sm8450         |     --device \"Samsung Galaxy S22 5G\" --device-os 13     |\n",
      "|        QCS8450 (Proxy)        |  Android 13  | Qualcomm |    Xr   |               qualcomm-qcs8450-proxy              |        --device \"QCS8450 (Proxy)\" --device-os 13        |\n",
      "|       XR2 Gen 2 (Proxy)       |  Android 13  | Qualcomm |    Xr   |               qualcomm-qcs8450-proxy              |       --device \"XR2 Gen 2 (Proxy)\" --device-os 13       |\n",
      "|  Samsung Galaxy S23 (Family)  |  Android 13  | Samsung  |  Phone  |         qualcomm-snapdragon-8gen2, sm8550         |  --device \"Samsung Galaxy S23 (Family)\" --device-os 13  |\n",
      "|         SA8650 (Proxy)        |  Android 13  | Qualcomm |   Auto  |               qualcomm-sa8650p-proxy              |         --device \"SA8650 (Proxy)\" --device-os 13        |\n",
      "|         SA8775 (Proxy)        |  Android 13  | Qualcomm |   Auto  |               qualcomm-sa8775p-proxy              |         --device \"SA8775 (Proxy)\" --device-os 13        |\n",
      "|         SA8255 (Proxy)        |  Android 13  | Qualcomm |   Auto  |               qualcomm-sa8255p-proxy              |         --device \"SA8255 (Proxy)\" --device-os 13        |\n",
      "|       Samsung Galaxy S23      |  Android 13  | Samsung  |  Phone  |         qualcomm-snapdragon-8gen2, sm8550         |       --device \"Samsung Galaxy S23\" --device-os 13      |\n",
      "|      Samsung Galaxy S23+      |  Android 13  | Samsung  |  Phone  |         qualcomm-snapdragon-8gen2, sm8550         |      --device \"Samsung Galaxy S23+\" --device-os 13      |\n",
      "|    Samsung Galaxy S23 Ultra   |  Android 13  | Samsung  |  Phone  |         qualcomm-snapdragon-8gen2, sm8550         |    --device \"Samsung Galaxy S23 Ultra\" --device-os 13   |\n",
      "|         Google Pixel 7        |  Android 14  |  Google  |  Phone  |                  google-tensor-g2                 |         --device \"Google Pixel 7\" --device-os 14        |\n",
      "|    Google Pixel 8 (Family)    |  Android 14  |  Google  |  Phone  |                  google-tensor-g3                 |    --device \"Google Pixel 8 (Family)\" --device-os 14    |\n",
      "|         Google Pixel 8        |  Android 14  |  Google  |  Phone  |                  google-tensor-g3                 |         --device \"Google Pixel 8\" --device-os 14        |\n",
      "|       Google Pixel 8 Pro      |  Android 14  |  Google  |  Phone  |                  google-tensor-g3                 |       --device \"Google Pixel 8 Pro\" --device-os 14      |\n",
      "|  Samsung Galaxy S24 (Family)  |  Android 14  | Samsung  |  Phone  |         qualcomm-snapdragon-8gen3, sm8650         |  --device \"Samsung Galaxy S24 (Family)\" --device-os 14  |\n",
      "|       Samsung Galaxy S24      |  Android 14  | Samsung  |  Phone  |         qualcomm-snapdragon-8gen3, sm8650         |       --device \"Samsung Galaxy S24\" --device-os 14      |\n",
      "|    Samsung Galaxy S24 Ultra   |  Android 14  | Samsung  |  Phone  |         qualcomm-snapdragon-8gen3, sm8650         |    --device \"Samsung Galaxy S24 Ultra\" --device-os 14   |\n",
      "|      Samsung Galaxy S24+      |  Android 14  | Samsung  |  Phone  |         qualcomm-snapdragon-8gen3, sm8650         |      --device \"Samsung Galaxy S24+\" --device-os 14      |\n",
      "|    Google Pixel 9 (Family)    |  Android 15  |  Google  |  Phone  |                  google-tensor-g4                 |    --device \"Google Pixel 9 (Family)\" --device-os 15    |\n",
      "|         Google Pixel 9        |  Android 15  |  Google  |  Phone  |                  google-tensor-g4                 |         --device \"Google Pixel 9\" --device-os 15        |\n",
      "|       Google Pixel 9 Pro      |  Android 15  |  Google  |  Phone  |                  google-tensor-g4                 |       --device \"Google Pixel 9 Pro\" --device-os 15      |\n",
      "|     Google Pixel 9 Pro XL     |  Android 15  |  Google  |  Phone  |                  google-tensor-g4                 |     --device \"Google Pixel 9 Pro XL\" --device-os 15     |\n",
      "|       Samsung Galaxy S25      |  Android 15  | Samsung  |  Phone  | qualcomm-snapdragon-8-elite-for-galaxy, sm8750-ac |       --device \"Samsung Galaxy S25\" --device-os 15      |\n",
      "|    Samsung Galaxy S25 Ultra   |  Android 15  | Samsung  |  Phone  | qualcomm-snapdragon-8-elite-for-galaxy, sm8750-ac |    --device \"Samsung Galaxy S25 Ultra\" --device-os 15   |\n",
      "|      Samsung Galaxy S25+      |  Android 15  | Samsung  |  Phone  | qualcomm-snapdragon-8-elite-for-galaxy, sm8750-ac |      --device \"Samsung Galaxy S25+\" --device-os 15      |\n",
      "|  Samsung Galaxy S25 (Family)  |  Android 15  | Samsung  |  Phone  | qualcomm-snapdragon-8-elite-for-galaxy, sm8750-ac |  --device \"Samsung Galaxy S25 (Family)\" --device-os 15  |\n",
      "|    Snapdragon 8cx Gen 3 CRD   |  Windows 11  | Qualcomm | Compute |        qualcomm-snapdragon-8cxgen3, sc8280x       |    --device \"Snapdragon 8cx Gen 3 CRD\" --device-os 11   |\n",
      "|     Snapdragon X Elite CRD    |  Windows 11  | Qualcomm | Compute |       qualcomm-snapdragon-x-elite, sc8380xp       |     --device \"Snapdragon X Elite CRD\" --device-os 11    |\n",
      "|  Snapdragon X Plus 8-Core CRD |  Windows 11  | Qualcomm | Compute |    qualcomm-snapdragon-x-plus-8-core, sc8340xp    |  --device \"Snapdragon X Plus 8-Core CRD\" --device-os 11 |\n",
      "|     Snapdragon 8 Gen 3 QRD    |  Android 14  | Qualcomm |  Phone  |         qualcomm-snapdragon-8gen3, sm8650         |     --device \"Snapdragon 8 Gen 3 QRD\" --device-os 14    |\n",
      "|     Snapdragon 8 Elite QRD    |  Android 15  | Qualcomm |  Phone  |        qualcomm-snapdragon-8-elite, sm8750        |     --device \"Snapdragon 8 Elite QRD\" --device-os 15    |\n",
      "|          SA8295P ADP          |  Android 14  | Qualcomm |   Auto  |                  qualcomm-sa8295p                 |          --device \"SA8295P ADP\" --device-os 14          |\n",
      "|          SA8775P ADP          |  Android 14  | Qualcomm |   Auto  |                  qualcomm-sa8775p                 |          --device \"SA8775P ADP\" --device-os 14          |\n",
      "|          SA7255P ADP          |  Android 14  | Qualcomm |   Auto  |                  qualcomm-sa7255p                 |          --device \"SA7255P ADP\" --device-os 14          |\n",
      "|        QCS9075 (Proxy)        |  Android 14  | Qualcomm |   Iot   |               qualcomm-qcs9075-proxy              |        --device \"QCS9075 (Proxy)\" --device-os 14        |\n",
      "|        QCS8275 (Proxy)        |  Android 14  | Qualcomm |   Iot   |               qualcomm-qcs8275-proxy              |        --device \"QCS8275 (Proxy)\" --device-os 14        |\n",
      "|      Dragonwing RB3 Gen 2     | Qc_Linux 1.3 | Qualcomm |   Iot   |                  qualcomm-qcs6490                 |     --device \"Dragonwing RB3 Gen 2\" --device-os 1.3     |\n",
      "|       Dragonwing QCS8550      | Qc_Linux 1.0 | Qualcomm |   Iot   |                  qualcomm-qcs8550                 |      --device \"Dragonwing QCS8550\" --device-os 1.0      |\n",
      "+-------------------------------+--------------+----------+---------+---------------------------------------------------+---------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!qai-hub list-devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9880795a",
   "metadata": {},
   "source": [
    "# Ideal Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab66147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading tmpag7uif2y.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 13.9M/13.9M [00:02<00:00, 5.34MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (j56zzyz0g) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/j56zzyz0g/\n",
      "\n",
      "Waiting for compile job (j56zzyz0g) completion. Type Ctrl+C to stop waiting at any time.\n",
      "    âœ… SUCCESS                          \u0007\n",
      "Scheduled profile job (jg9yylwl5) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jg9yylwl5/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading dataset: 154kB [00:00, 384kB/s]                    <?, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled inference job (jp1ww4e2g) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jp1ww4e2g/\n",
      "\n",
      "Waiting for inference job (jp1ww4e2g) completion. Type Ctrl+C to stop waiting at any time.\n",
      "    â³ PROVISIONING_DEVICE   â–ˆâ–‘â–‘ 1/3 .. \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Run inference using the on-device model on the input image\u001b[39;00m\n\u001b[0;32m     44\u001b[0m inference_job \u001b[38;5;241m=\u001b[39m hub\u001b[38;5;241m.\u001b[39msubmit_inference_job(\n\u001b[0;32m     45\u001b[0m     model\u001b[38;5;241m=\u001b[39mtarget_model,\n\u001b[0;32m     46\u001b[0m     device\u001b[38;5;241m=\u001b[39mhub\u001b[38;5;241m.\u001b[39mDevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSnapdragon X Elite CRD\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     47\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(image\u001b[38;5;241m=\u001b[39m[input_array]),\n\u001b[0;32m     48\u001b[0m )\n\u001b[1;32m---> 49\u001b[0m on_device_output \u001b[38;5;241m=\u001b[39m \u001b[43minference_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_output_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Step 5: Post-processing the on-device output\u001b[39;00m\n\u001b[0;32m     52\u001b[0m output_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(on_device_output\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\qai_hub\\client.py:3138\u001b[0m, in \u001b[0;36mInferenceJob.download_output_data\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m   3120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdownload_output_data\u001b[39m(\n\u001b[0;32m   3121\u001b[0m     \u001b[38;5;28mself\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DatasetEntries \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3124\u001b[0m \u001b[38;5;124;03m    Returns the downloaded output data, either in memory or as a h5f file.\u001b[39;00m\n\u001b[0;32m   3125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3136\u001b[0m \u001b[38;5;124;03m        The downloaded output data, filename, or None if the job failed.\u001b[39;00m\n\u001b[0;32m   3137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3138\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\qai_hub\\client.py:3105\u001b[0m, in \u001b[0;36mInferenceJob.get_output_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3094\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3095\u001b[0m \u001b[38;5;124;03mReturns the output dataset for a job.\u001b[39;00m\n\u001b[0;32m   3096\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3102\u001b[0m \u001b[38;5;124;03m    The output data if the job succeeded\u001b[39;00m\n\u001b[0;32m   3103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs:\n\u001b[1;32m-> 3105\u001b[0m     status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[0;32m   3107\u001b[0m         result \u001b[38;5;241m=\u001b[39m _api_call(api\u001b[38;5;241m.\u001b[39mget_job_results, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_owner\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_id)\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\qai_hub\\client.py:2265\u001b[0m, in \u001b[0;36mJob.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   2260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mand\u001b[39;00m time_elapsed \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m timeout:\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[0;32m   2262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m job (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) completion timed out after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_elapsed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2263\u001b[0m     )\n\u001b[1;32m-> 2265\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_seconds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2266\u001b[0m time_elapsed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sleep_seconds\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_elapsed \u001b[38;5;241m%\u001b[39m Job\u001b[38;5;241m.\u001b[39m_polling_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import qai_hub as hub\n",
    "import torch\n",
    "from torchvision.models import mobilenet_v2\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Using pre-trained MobileNet\n",
    "torch_model = mobilenet_v2(pretrained=True)\n",
    "torch_model.eval()\n",
    "\n",
    "# Step 1: Trace model\n",
    "input_shape = (1, 3, 224, 224)\n",
    "example_input = torch.rand(input_shape)\n",
    "traced_torch_model = torch.jit.trace(torch_model, example_input)\n",
    "\n",
    "# Step 2: Compile model\n",
    "compile_job = hub.submit_compile_job(\n",
    "    model=traced_torch_model,\n",
    "    device=hub.Device(\"Snapdragon X Elite CRD\"),\n",
    "    input_specs=dict(image=input_shape),\n",
    "    options=\"--target_runtime onnx\",\n",
    ")\n",
    "\n",
    "# Step 3: Profile on cloud-hosted device\n",
    "target_model = compile_job.get_target_model()\n",
    "profile_job = hub.submit_profile_job(\n",
    "    model=target_model,\n",
    "    device=hub.Device(\"Snapdragon X Elite CRD\"),\n",
    ")\n",
    "\n",
    "# Step 4: Run inference on cloud-hosted device\n",
    "sample_image_url = (\n",
    "    \"https://qaihub-public-assets.s3.us-west-2.amazonaws.com/apidoc/input_image1.jpg\"\n",
    ")\n",
    "response = requests.get(sample_image_url, stream=True)\n",
    "response.raw.decode_content = True\n",
    "image = Image.open(response.raw).resize((224, 224))\n",
    "input_array = np.expand_dims(\n",
    "    np.transpose(np.array(image, dtype=np.float32) / 255.0, (2, 0, 1)), axis=0\n",
    ")\n",
    "\n",
    "# Run inference using the on-device model on the input image\n",
    "inference_job = hub.submit_inference_job(\n",
    "    model=target_model,\n",
    "    device=hub.Device(\"Snapdragon X Elite CRD\"),\n",
    "    inputs=dict(image=[input_array]),\n",
    ")\n",
    "on_device_output = inference_job.download_output_data()\n",
    "\n",
    "# Step 5: Post-processing the on-device output\n",
    "output_name = list(on_device_output.keys())[0]\n",
    "out = on_device_output[output_name][0]\n",
    "on_device_probabilities = np.exp(out) / np.sum(np.exp(out), axis=1)\n",
    "\n",
    "# Read the class labels for imagenet\n",
    "sample_classes = \"https://qaihub-public-assets.s3.us-west-2.amazonaws.com/apidoc/imagenet_classes.txt\"\n",
    "response = requests.get(sample_classes, stream=True)\n",
    "response.raw.decode_content = True\n",
    "categories = [str(s.strip()) for s in response.raw]\n",
    "\n",
    "# Print top five predictions for the on-device model\n",
    "print(\"Top-5 On-Device predictions:\")\n",
    "top5_classes = np.argsort(on_device_probabilities[0], axis=0)[-5:]\n",
    "for c in reversed(top5_classes):\n",
    "    print(f\"{c} {categories[c]:20s} {on_device_probabilities[0][c]:>6.1%}\")\n",
    "\n",
    "# Step 6: Download model\n",
    "target_model = compile_job.get_target_model()\n",
    "target_model.download(\"mobilenet_v2.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8132b99c",
   "metadata": {},
   "source": [
    "# TryOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e042ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\qc_de/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing 16 frames from webcam...\n",
      "Exporting model to x3d_xs_webcam.onnx...\n",
      "âœ… Model exported successfully: x3d_xs_webcam.onnx\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import torch.onnx\n",
    "\n",
    "# 1. Load pretrained X3D-XS model\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo\", \"x3d_xs\", pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# 2. Define transforms (Image â†’ Tensor)\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225])\n",
    "])\n",
    "\n",
    "# 3. Capture 16 frames from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "frames = []\n",
    "\n",
    "print(\"Capturing 16 frames from webcam...\")\n",
    "while len(frames) < 16:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(rgb)\n",
    "    tensor = transform(pil_img)\n",
    "    frames.append(tensor)\n",
    "cap.release()\n",
    "\n",
    "# 4. Format: (T, C, H, W) â†’ (1, C, T, H, W)\n",
    "video_tensor = torch.stack(frames, dim=0).permute(1, 0, 2, 3).unsqueeze(0)\n",
    "\n",
    "# 5. Export to ONNX\n",
    "onnx_path = \"x3d_xs_webcam.onnx\"\n",
    "print(f\"Exporting model to {onnx_path}...\")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    video_tensor,  # (1, 3, 16, 224, 224)\n",
    "    \"x3d_xs_webcam_static.onnx\",\n",
    "    input_names=[\"video\"],\n",
    "    output_names=[\"logits\"],\n",
    "    opset_version=13,\n",
    "    dynamic_axes=None  # âœ… Remove dynamic_axes\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model exported successfully: {onnx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31fc2cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading x3d_xs_webcam_static.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 14.5M/14.5M [00:02<00:00, 6.44MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (jgzjj37op) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jgzjj37op/\n",
      "\n",
      "Waiting for compile job (jgzjj37op) completion. Type Ctrl+C to stop waiting at any time.\n",
      "    âœ… SUCCESS                          \u0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading dataset: 1.90MB [00:00, 4.10MB/s]                            5.03MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled inference job (jp1ww488g) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jp1ww488g/\n",
      "\n",
      "Waiting for inference job (jp1ww488g) completion. Type Ctrl+C to stop waiting at any time.\n",
      "    âœ… SUCCESS                          \u0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tmpn3dkvmvm.h5: 100%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 13.9k/13.9k [00:00<00:00, 14.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 predicted class indices:\n",
      "Class 196 : 68.2%\n",
      "Class  36 : 3.0%\n",
      "Class  79 : 2.6%\n",
      "Class 392 : 2.5%\n",
      "Class 316 : 1.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from qai_hub import submit_compile_job, submit_inference_job, Device\n",
    "\n",
    "# Submit compile job\n",
    "compile_job = submit_compile_job(\n",
    "    model=\"x3d_xs_webcam_static.onnx\",\n",
    "    device=Device(\"Snapdragon X Elite CRD\"),\n",
    "    options=\"--target_runtime onnx\"\n",
    ")\n",
    "target_model = compile_job.get_target_model()\n",
    "\n",
    "# Run inference\n",
    "inference_job = submit_inference_job(\n",
    "    model=target_model,\n",
    "    device=Device(\"Snapdragon X Elite CRD\"),\n",
    "    inputs={\"video\": [video_tensor.numpy()]},\n",
    ")\n",
    "result = inference_job.download_output_data()\n",
    "output_logits = list(result.values())[0][0]\n",
    "probs = torch.tensor(output_logits).softmax(dim=-1)\n",
    "\n",
    "# Print top predictions\n",
    "top5 = torch.topk(probs, 5)\n",
    "print(\"Top-5 predicted class indices:\")\n",
    "for idx, prob in zip(top5.indices[0], top5.values[0]):\n",
    "    print(f\"Class {idx.item():3d} : {prob.item()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c219aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download the label file\n",
    "url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Step 2: Load and invert the mapping\n",
    "class_map = json.loads(response.text)\n",
    "index_to_label = {v: k for k, v in class_map.items()}\n",
    "\n",
    "# Step 3: Create a list where index matches label position\n",
    "labels = [index_to_label[i] for i in range(len(index_to_label))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8bbfa",
   "metadata": {},
   "source": [
    "## Webcam Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c33bb93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ Starting real-time activity recognition...\n",
      "beatboxing 73.50980639457703\n",
      "playing harmonica 7.458873838186264\n",
      "shaking head 5.060039833188057\n",
      "cracking neck 4.629659280180931\n",
      "smoking 0.9155870415270329\n",
      "beatboxing 64.70531225204468\n",
      "counting money 7.534674555063248\n",
      "singing 4.750609770417213\n",
      "playing harmonica 3.7736229598522186\n",
      "fixing hair 2.648218162357807\n",
      "beatboxing 26.610475778579712\n",
      "singing 11.11048236489296\n",
      "fixing hair 8.783400058746338\n",
      "answering questions 6.3661277294158936\n",
      "playing ukulele 5.524526908993721\n",
      "beatboxing 43.91208291053772\n",
      "playing harmonica 17.475034296512604\n",
      "whistling 2.5240490213036537\n",
      "playing recorder 2.444513700902462\n",
      "answering questions 2.2485192865133286\n",
      "tapping guitar 8.45736563205719\n",
      "playing ukulele 5.776163190603256\n",
      "playing guitar 3.9676733314990997\n",
      "playing bass guitar 2.345132641494274\n",
      "playing recorder 2.1889496594667435\n",
      "beatboxing 24.648168683052063\n",
      "playing harmonica 16.092631220817566\n",
      "finger snapping 8.953815698623657\n",
      "playing recorder 8.083099126815796\n",
      "whistling 6.841965019702911\n",
      "beatboxing 82.23244547843933\n",
      "crying 3.7622645497322083\n",
      "filling eyebrows 3.257894143462181\n",
      "blowing nose 2.927256003022194\n",
      "fixing hair 1.7977599054574966\n",
      "beatboxing 82.27573037147522\n",
      "massaging person's head 2.384553477168083\n",
      "shaking head 2.326103299856186\n",
      "fixing hair 2.271146699786186\n",
      "crying 1.9405150786042213\n",
      "massaging person's head 7.966700196266174\n",
      "beatboxing 5.764669179916382\n",
      "sticking tongue out 4.220717027783394\n",
      "brushing hair 3.7559904158115387\n",
      "crying 3.0840355902910233\n",
      "beatboxing 13.712626695632935\n",
      "playing harmonica 8.961595594882965\n",
      "massaging person's head 4.836548864841461\n",
      "crying 3.463233634829521\n",
      "whistling 3.019467741250992\n",
      "tapping guitar 6.226674094796181\n",
      "playing ukulele 6.103726848959923\n",
      "playing guitar 3.911535069346428\n",
      "whistling 2.588353119790554\n",
      "playing bass guitar 2.394893765449524\n",
      "playing ukulele 6.344165652990341\n",
      "tapping guitar 6.114859879016876\n",
      "playing guitar 4.182892665266991\n",
      "filling eyebrows 2.961583621799946\n",
      "whistling 2.621947042644024\n",
      "beatboxing 42.929407954216\n",
      "fixing hair 5.449997633695602\n",
      "singing 5.292597785592079\n",
      "crying 3.3548839390277863\n",
      "shaking head 2.9314300045371056\n",
      "beatboxing 41.729506850242615\n",
      "finger snapping 12.458664178848267\n",
      "massaging person's head 9.021653234958649\n",
      "whistling 2.6038741692900658\n",
      "headbanging 2.515116333961487\n",
      "massaging person's head 22.985389828681946\n",
      "beatboxing 12.628179788589478\n",
      "finger snapping 7.6600827276706696\n",
      "playing ukulele 7.098803669214249\n",
      "drumming fingers 6.485620886087418\n",
      "playing harmonica 11.016806215047836\n",
      "reading book 6.391812860965729\n",
      "massaging person's head 6.351634860038757\n",
      "playing ukulele 6.295027583837509\n",
      "fixing hair 3.926292434334755\n",
      "massaging person's head 6.7733414471149445\n",
      "playing ukulele 5.4815612733364105\n",
      "reading book 3.643302246928215\n",
      "playing harmonica 3.3996444195508957\n",
      "brushing hair 3.170134127140045\n",
      "playing ukulele 4.905354231595993\n",
      "massaging person's head 4.144860431551933\n",
      "brushing hair 3.5860061645507812\n",
      "playing harmonica 2.6104703545570374\n",
      "reading book 2.259735018014908\n",
      "massaging person's head 9.630892425775528\n",
      "brushing hair 7.079658657312393\n",
      "playing ukulele 6.659968197345734\n",
      "fixing hair 6.00242055952549\n",
      "curling hair 3.822990134358406\n",
      "massaging person's head 98.44842553138733\n",
      "fixing hair 0.9213371202349663\n",
      "curling hair 0.1440143445506692\n",
      "braiding hair 0.06772712804377079\n",
      "brushing hair 0.05771023570559919\n",
      "massaging person's head 96.21942043304443\n",
      "fixing hair 0.8058262057602406\n",
      "filling eyebrows 0.32400134950876236\n",
      "brushing hair 0.25205397978425026\n",
      "curling hair 0.22557934280484915\n",
      "massaging person's head 99.93428587913513\n",
      "fixing hair 0.023210610379464924\n",
      "waxing eyebrows 0.006859708082629368\n",
      "applying cream 0.0053741921874461696\n",
      "singing 0.0029363563953666016\n",
      "massaging person's head 99.1418719291687\n",
      "fixing hair 0.36625268403440714\n",
      "applying cream 0.05511538474820554\n",
      "brushing hair 0.040108844405040145\n",
      "curling hair 0.034888318623416126\n",
      "beatboxing 11.174532771110535\n",
      "whistling 10.131386667490005\n",
      "playing ukulele 9.113827347755432\n",
      "answering questions 7.521407306194305\n",
      "playing harmonica 4.286680743098259\n",
      "playing ukulele 10.944728553295135\n",
      "massaging person's head 8.77857506275177\n",
      "beatboxing 6.240232288837433\n",
      "answering questions 5.318016931414604\n",
      "whistling 5.29664009809494\n",
      "playing ukulele 9.804430603981018\n",
      "tapping guitar 7.188911736011505\n",
      "massaging person's head 5.728844180703163\n",
      "playing harmonica 2.403574623167515\n",
      "playing guitar 2.352473698556423\n",
      "massaging person's head 47.810474038124084\n",
      "whistling 15.575169026851654\n",
      "singing 4.248873516917229\n",
      "playing clarinet 2.5103341788053513\n",
      "answering questions 2.39312294870615\n",
      "beatboxing 19.77328062057495\n",
      "singing 10.62764972448349\n",
      "answering questions 10.227960348129272\n",
      "massaging person's head 10.21391898393631\n",
      "whistling 6.8153344094753265\n",
      "playing ukulele 12.933747470378876\n",
      "massaging person's head 10.270673036575317\n",
      "answering questions 8.514025062322617\n",
      "beatboxing 7.689675688743591\n",
      "singing 5.267377570271492\n",
      "massaging person's head 12.268110364675522\n",
      "playing ukulele 6.828710436820984\n",
      "beatboxing 4.845384880900383\n",
      "reading book 4.22046035528183\n",
      "fixing hair 3.935457020998001\n",
      "playing ukulele 8.797169476747513\n",
      "massaging person's head 8.660823106765747\n",
      "tapping guitar 3.4209951758384705\n",
      "beatboxing 3.4093547612428665\n",
      "playing harmonica 3.041743114590645\n",
      "beatboxing 15.606096386909485\n",
      "shaking head 10.791697353124619\n",
      "playing ukulele 6.868264079093933\n",
      "massaging person's head 5.0925955176353455\n",
      "whistling 4.426838830113411\n",
      "beatboxing 18.627725541591644\n",
      "shaking head 16.9389545917511\n",
      "playing ukulele 5.94320148229599\n",
      "whistling 4.830272868275642\n",
      "playing harmonica 3.931649401783943\n",
      "beatboxing 14.316897094249725\n",
      "shaking head 12.757335603237152\n",
      "playing ukulele 5.960872769355774\n",
      "playing harmonica 5.288590490818024\n",
      "recording music 5.045546963810921\n",
      "massaging person's head 70.14883160591125\n",
      "brushing hair 2.8358884155750275\n",
      "fixing hair 2.5364981964230537\n",
      "shaking head 1.8906094133853912\n",
      "playing ukulele 1.796976663172245\n",
      "massaging person's head 16.203893721103668\n",
      "playing ukulele 7.496592402458191\n",
      "whistling 6.8696849048137665\n",
      "singing 3.8561303168535233\n",
      "brushing hair 3.7266552448272705\n",
      "massaging person's head 15.327773988246918\n",
      "fixing hair 7.214353233575821\n",
      "brushing hair 4.930545762181282\n",
      "braiding hair 4.151943698525429\n",
      "whistling 4.077379032969475\n",
      "massaging person's head 66.43694639205933\n",
      "whistling 9.83814150094986\n",
      "shaking head 1.7631357535719872\n",
      "playing harmonica 1.3789166696369648\n",
      "brushing hair 1.3163380324840546\n",
      "massaging person's head 98.79236817359924\n",
      "fixing hair 0.32331086695194244\n",
      "brushing hair 0.20164570305496454\n",
      "whistling 0.09041680605150759\n",
      "braiding hair 0.05028329323977232\n",
      "massaging person's head 8.558932691812515\n",
      "playing ukulele 4.740450903773308\n",
      "brushing hair 3.4757904708385468\n",
      "fixing hair 2.8060883283615112\n",
      "reading book 2.2556284442543983\n",
      "playing ukulele 12.250808626413345\n",
      "reading book 5.194402858614922\n",
      "massaging person's head 4.143992066383362\n",
      "tapping guitar 4.073561728000641\n",
      "playing harmonica 3.1452808529138565\n",
      "whistling 14.098086953163147\n",
      "playing ukulele 12.685973942279816\n",
      "beatboxing 7.490983605384827\n",
      "massaging person's head 7.169356197118759\n",
      "singing 3.997466340661049\n",
      "playing ukulele 7.907946407794952\n",
      "massaging person's head 7.155099511146545\n",
      "playing harmonica 4.316353052854538\n",
      "whistling 3.6993280053138733\n",
      "singing 3.1225793063640594\n",
      "massaging person's head 7.9123154282569885\n",
      "playing ukulele 6.230765953660011\n",
      "playing harmonica 4.341377317905426\n",
      "playing trumpet 2.9157178476452827\n",
      "answering questions 2.5824440643191338\n",
      "massaging person's head 6.967414170503616\n",
      "playing ukulele 5.697081610560417\n",
      "playing harmonica 3.5802483558654785\n",
      "playing trumpet 3.19458544254303\n",
      "answering questions 2.6990817859768867\n",
      "massaging person's head 5.733451992273331\n",
      "playing ukulele 5.6706637144088745\n",
      "playing harmonica 3.504132106900215\n",
      "playing trumpet 3.1669221818447113\n",
      "tapping guitar 2.554849348962307\n",
      "massaging person's head 5.241236463189125\n",
      "playing ukulele 5.2220504730939865\n",
      "playing harmonica 3.363727778196335\n",
      "playing trumpet 3.2283246517181396\n",
      "tapping guitar 2.3493053391575813\n",
      "playing ukulele 4.9544718116521835\n",
      "massaging person's head 4.914005845785141\n",
      "playing trumpet 3.5923290997743607\n",
      "playing harmonica 3.3888470381498337\n",
      "tapping guitar 2.360900118947029\n",
      "beatboxing 38.93704116344452\n",
      "finger snapping 6.677348166704178\n",
      "sign language interpreting 5.055411532521248\n",
      "playing ukulele 4.995542019605637\n",
      "answering questions 4.7917429357767105\n",
      "beatboxing 18.259306252002716\n",
      "massaging person's head 12.158884108066559\n",
      "fixing hair 11.523497849702835\n",
      "finger snapping 7.471907138824463\n",
      "drumming fingers 5.860104039311409\n",
      "beatboxing 21.78700715303421\n",
      "fixing hair 8.723317086696625\n",
      "curling hair 7.005830854177475\n",
      "massaging person's head 5.381883308291435\n",
      "tapping pen 5.153242871165276\n",
      "beatboxing 32.83007740974426\n",
      "finger snapping 4.750283434987068\n",
      "massaging person's head 4.606721922755241\n",
      "fixing hair 3.0348945409059525\n",
      "answering questions 2.609788440167904\n",
      "beatboxing 31.827864050865173\n",
      "finger snapping 12.369375675916672\n",
      "massaging person's head 8.25231596827507\n",
      "fixing hair 6.704527884721756\n",
      "brushing hair 3.6328617483377457\n",
      "beatboxing 21.03266268968582\n",
      "finger snapping 11.350416392087936\n",
      "massaging person's head 10.807447880506516\n",
      "drumming fingers 7.37837627530098\n",
      "tapping guitar 7.371392101049423\n",
      "beatboxing 28.138038516044617\n",
      "massaging person's head 21.1281880736351\n",
      "fixing hair 15.550997853279114\n",
      "finger snapping 9.492029994726181\n",
      "shaking head 3.4843746572732925\n",
      "beatboxing 21.09406441450119\n",
      "massaging person's head 9.185810387134552\n",
      "tapping guitar 6.605633348226547\n",
      "sticking tongue out 5.84869310259819\n",
      "finger snapping 5.545220896601677\n",
      "beatboxing 46.86359465122223\n",
      "shaking head 44.74421441555023\n",
      "whistling 0.8742118254303932\n",
      "headbanging 0.7911249063909054\n",
      "recording music 0.7351123727858067\n",
      "beatboxing 25.986266136169434\n",
      "shaking head 11.625518649816513\n",
      "playing ukulele 7.031188905239105\n",
      "tapping guitar 5.951898917555809\n",
      "sticking tongue out 3.454664722084999\n",
      "beatboxing 36.89994215965271\n",
      "shaking head 22.761477530002594\n",
      "sticking tongue out 4.656511545181274\n",
      "playing ukulele 3.197978436946869\n",
      "whistling 2.7172615751624107\n",
      "shaking head 70.17534971237183\n",
      "beatboxing 22.120730578899384\n",
      "headbanging 1.5219160355627537\n",
      "sticking tongue out 0.5632940214127302\n",
      "playing harmonica 0.4060372244566679\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load class labels (Kinetics-400)\n",
    "label_url = \"https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt\"\n",
    "labels = requests.get(label_url).text.strip().split(\"\\n\")\n",
    "\n",
    "# Preprocessing pipeline\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize([0.45, 0.45, 0.45], [0.225, 0.225, 0.225])\n",
    "])\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(\"x3d_xs_webcam_static.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# Webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "frames = []\n",
    "\n",
    "print(\"ðŸŽ¥ Starting real-time activity recognition...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Convert and preprocess\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(rgb)\n",
    "    tensor_img = transform(pil_img)\n",
    "    frames.append(tensor_img)\n",
    "\n",
    "    if len(frames) == 16:\n",
    "        # Prepare input: (T, C, H, W) â†’ (1, C, T, H, W)\n",
    "        video_tensor = torch.stack(frames).permute(1, 0, 2, 3).unsqueeze(0).numpy()\n",
    "        frames = []  # Reset buffer\n",
    "\n",
    "        # Inference\n",
    "        outputs = session.run(None, {input_name: video_tensor})\n",
    "        probs = torch.tensor(outputs[0]).softmax(dim=-1)[0]\n",
    "        top5 = torch.topk(probs, 5)\n",
    "\n",
    "        # Draw predictions on frame\n",
    "        for i, (idx, prob) in enumerate(zip(top5.indices, top5.values)):\n",
    "            label = labels[idx.item()]\n",
    "            text = f\"{label}: {prob.item()*100:.1f}%\"\n",
    "            cv2.putText(frame, text, (10, 30 + i*25),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            print(label, prob.item() * 100)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"Real-Time Activity Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37789128",
   "metadata": {},
   "source": [
    "## Webcam: Average 5 Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a61708b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ Starting real-time activity recognition...\n",
      "massaging person's head 31.671777367591858\n",
      "massaging person's head 35.23405194282532\n",
      "massaging person's head 30.698195099830627\n",
      "massaging person's head 35.83304286003113\n",
      "massaging person's head 40.16903042793274\n",
      "massaging person's head 39.080601930618286\n",
      "massaging person's head 35.29893159866333\n",
      "finger snapping 30.935224890708923\n",
      "finger snapping 40.45135974884033\n",
      "finger snapping 45.773300528526306\n",
      "finger snapping 38.88634741306305\n",
      "finger snapping 37.03685700893402\n",
      "finger snapping 40.65384864807129\n",
      "finger snapping 34.5172256231308\n",
      "finger snapping 33.37318301200867\n",
      "finger snapping 30.45106530189514\n",
      "massaging person's head 33.14814567565918\n",
      "massaging person's head 33.16800892353058\n",
      "finger snapping 33.7223619222641\n",
      "finger snapping 34.03366208076477\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import requests\n",
    "import time\n",
    "\n",
    "last_prediction = []       # stores list of (label, prob)\n",
    "last_prediction_time = 0   # timestamp of last update\n",
    "display_duration = 2       # seconds to keep text on screen\n",
    "\n",
    "\n",
    "# Load class labels (Kinetics-400)\n",
    "label_url = \"https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt\"\n",
    "labels = requests.get(label_url).text.strip().split(\"\\n\")\n",
    "\n",
    "# Preprocessing pipeline\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize([0.45, 0.45, 0.45], [0.225, 0.225, 0.225])\n",
    "])\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(\"x3d_xs_webcam_static.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# Webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "frames = []\n",
    "\n",
    "print(\"ðŸŽ¥ Starting real-time activity recognition...\")\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "rolling_window = deque(maxlen=5)  # last 5 inference results\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Convert and preprocess\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(rgb)\n",
    "    tensor_img = transform(pil_img)\n",
    "    frames.append(tensor_img)\n",
    "\n",
    "    if len(frames) == 16:\n",
    "        # Prepare input: (T, C, H, W) â†’ (1, C, T, H, W)\n",
    "        video_tensor = torch.stack(frames).permute(1, 0, 2, 3).unsqueeze(0).numpy()\n",
    "        frames = []  # Reset buffer\n",
    "\n",
    "        # Run ONNX inference\n",
    "        outputs = session.run(None, {input_name: video_tensor})\n",
    "        probs = torch.tensor(outputs[0]).softmax(dim=-1)[0]\n",
    "\n",
    "        # Add to rolling window\n",
    "        rolling_window.append(probs)\n",
    "        if rolling_window:\n",
    "            avg_probs = torch.stack(list(rolling_window)).mean(dim=0)\n",
    "            top5 = torch.topk(avg_probs, 1)\n",
    "            last_prediction = [(labels[idx.item()], prob.item()) for idx, prob in zip(top5.indices, top5.values)]\n",
    "            last_prediction_time = time.time()\n",
    "\n",
    "        if time.time() - last_prediction_time < display_duration:\n",
    "            for i, (label, prob) in enumerate(last_prediction):\n",
    "                if prob > 0.3:  # Optional: skip low confidence\n",
    "                    text = f\"{label}: {prob*100:.1f}%\"\n",
    "                    cv2.putText(frame, text, (10, 30 + i*25),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                    print(label, prob * 100)\n",
    "        \n",
    "        # # Draw predictions on frame\n",
    "        # for i, (idx, prob) in enumerate(zip(top5.indices, top5.values)):\n",
    "        #     label = labels[idx.item()]\n",
    "        #     text = f\"{label}: {prob.item()*100:.1f}%\"\n",
    "        #     cv2.putText(frame, text, (10, 30 + i*25),\n",
    "        #                 cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        #     print(label, prob.item() * 100)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"Real-Time Activity Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c8c0d",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc48c7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video with human actions...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import requests\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# Load labels\n",
    "label_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "class_map = requests.get(label_url).json()\n",
    "labels = [None] * len(class_map)\n",
    "for name, idx in class_map.items():\n",
    "    labels[int(idx)] = name\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize([0.45]*3, [0.225]*3)\n",
    "])\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(\"x3d_xs_webcam_static.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# Rolling average\n",
    "rolling_window = deque(maxlen=5)\n",
    "display_duration = 2\n",
    "last_pred_time = 0\n",
    "last_prediction = []\n",
    "\n",
    "# Load video file with humans\n",
    "cap = cv2.VideoCapture(\"videos/driver-action-recognition.mp4\")\n",
    "\n",
    "frames = []\n",
    "print(\"Processing video with human actions...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess frame\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = transform(Image.fromarray(rgb))\n",
    "    frames.append(img)\n",
    "\n",
    "    # Inference every 16 frames\n",
    "    if len(frames) == 16:\n",
    "        video_tensor = torch.stack(frames).permute(1, 0, 2, 3).unsqueeze(0).numpy()\n",
    "        outputs = session.run(None, {input_name: video_tensor})\n",
    "        probs = torch.tensor(outputs[0]).softmax(dim=-1)[0]\n",
    "        rolling_window.append(probs)\n",
    "        avg_probs = torch.stack(list(rolling_window)).mean(dim=0)\n",
    "        top5 = torch.topk(avg_probs, 5)\n",
    "        last_prediction = [(labels[idx.item()], prob.item()) for idx, prob in zip(top5.indices, top5.values)]\n",
    "        last_pred_time = time.time()\n",
    "        frames.clear()\n",
    "\n",
    "    # Draw predictions if recent\n",
    "    if time.time() - last_pred_time < display_duration:\n",
    "        for i, (label, prob) in enumerate(last_prediction):\n",
    "            if prob > 0.3:\n",
    "                text = f\"{label}: {prob*100:.1f}%\"\n",
    "                cv2.putText(frame, text, (10, 30 + i*25),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow(\"Activity Recognition â€“ Video File\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ff59a",
   "metadata": {},
   "source": [
    "# VideoMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3aeb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ Starting real-time VideoMAE inference...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# video_mae model Source:\n",
    "# https://aihub.qualcomm.com/models/video_mae?searchTerm=action&domain=Computer+Vision&useCase=Video+Classification\n",
    "\n",
    "# Load VideoMAE ONNX model\n",
    "session = ort.InferenceSession(\"models/video_mae.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# Load class labels (Kinetics-400)\n",
    "label_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "labels_map = requests.get(label_url).json()\n",
    "index_to_label = {v: k for k, v in labels_map.items()}\n",
    "labels = [index_to_label[i] for i in range(len(index_to_label))]\n",
    "\n",
    "# Define transform\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Rolling average buffer\n",
    "rolling_window = deque(maxlen=5)\n",
    "display_duration = 2\n",
    "last_pred_time = 0\n",
    "last_prediction = []\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "frames = []\n",
    "\n",
    "print(\"ðŸŽ¥ Starting real-time VideoMAE inference...\")\n",
    "\n",
    "# Store last 50 predictions\n",
    "recent_predictions = deque(maxlen=50)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Preprocess frame\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(rgb)\n",
    "    tensor_img = transform(pil_img)\n",
    "    frames.append(tensor_img)\n",
    "\n",
    "    # Once we have 16 frames, run inference\n",
    "    if len(frames) == 16:\n",
    "        clip = torch.stack(frames).permute(1, 0, 2, 3).unsqueeze(0).numpy()  # (1, 3, 16, 224, 224)\n",
    "        frames.clear()\n",
    "\n",
    "        # Run ONNX inference\n",
    "        outputs = session.run(None, {input_name: clip})\n",
    "        probs = torch.tensor(outputs[0]).softmax(dim=-1)[0]\n",
    "        rolling_window.append(probs)\n",
    "\n",
    "        # Average predictions over buffer\n",
    "        avg_probs = torch.stack(list(rolling_window)).mean(dim=0)\n",
    "        top5 = torch.topk(avg_probs, 5)\n",
    "        # last_prediction = [(labels[idx.item()], prob.item()) for idx, prob in zip(top5.indices, top5.values)]\n",
    "        # last_pred_time = time.time()\n",
    "        \n",
    "        top5_results = [(labels[idx.item()], prob.item()) for idx, prob in zip(top5.indices, top5.values)]\n",
    "        last_prediction = top5_results\n",
    "        last_pred_time = time.time()\n",
    "\n",
    "        # Store only the top-1 label for analysis\n",
    "        top1_label = top5_results[0][0]\n",
    "        recent_predictions.append(top1_label)\n",
    "\n",
    "    # Draw predictions if within display window\n",
    "    if time.time() - last_pred_time < display_duration:\n",
    "        for i, (label, prob) in enumerate(last_prediction):\n",
    "            if prob > 0.3:\n",
    "                text = f\"{label}: {prob*100:.1f}%\"\n",
    "                cv2.putText(frame, text, (10, 30 + i*25),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"VideoMAE - Webcam Activity Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "with open(\"activity_predictions.txt\", \"w\") as f:\n",
    "    for label in recent_predictions:\n",
    "        f.write(label + \"\\n\")\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe37d3",
   "metadata": {},
   "source": [
    "# Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f128d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31.0\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "print(pkg_resources.get_distribution(\"qai-hub-models\").version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9fd6049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['allam_7b', 'aotgan', 'baichuan2_7b', 'beit', 'bgnet', 'bisenet', 'common.py', 'conditional_detr_resnet50', 'controlnet', 'convnext_base', 'convnext_tiny', 'ddrnet23_slim', 'deepbox', 'deeplabv3_plus_mobilenet', 'deeplabv3_resnet50', 'densenet121', 'depth_anything', 'depth_anything_v2', 'detr_resnet101', 'detr_resnet101_dc5', 'detr_resnet50', 'detr_resnet50_dc5', 'dla102x', 'easyocr', 'efficientnet_b0', 'efficientnet_b4', 'efficientnet_v2_s', 'efficientvit_b2_cls', 'efficientvit_l2_cls', 'efficientvit_l2_seg', 'esrgan', 'facemap_3dmm', 'face_attrib_net', 'face_det_lite', 'fastsam_s', 'fastsam_x', 'fcn_resnet50', 'ffnet_122ns_lowres', 'ffnet_40s', 'ffnet_54s', 'ffnet_78s', 'ffnet_78s_lowres', 'fomm', 'foot_track_net', 'gear_guard_net', 'googlenet', 'hrnet_pose', 'hrnet_w48_ocr', 'huggingface_wavlm_base_plus', 'ibm_granite_v3_1_8b_instruct', 'inception_v3', 'indus_1b', 'jais_6p7b_chat', 'lama_dilated', 'levit', 'litehrnet', 'llama_v2_7b_chat', 'llama_v3_1_8b_instruct', 'llama_v3_1_sea_lion_3_5_8b_r', 'llama_v3_2_3b_instruct', 'llama_v3_8b_instruct', 'llama_v3_taide_8b_chat', 'mask2former', 'mediapipe_face', 'mediapipe_hand', 'mediapipe_pose', 'mediapipe_selfie', 'midas', 'ministral_3b', 'mistral_3b', 'mistral_7b_instruct_v0_3', 'mnasnet05', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'mobilesam', 'mobile_vit', 'movenet', 'nasnet', 'nomic_embed_text', 'openai_clip', 'phi_3_5_mini_instruct', 'pidnet', 'plamo_1b', 'posenet_mobilenet', 'protocols.py', 'quicksrnetlarge', 'quicksrnetmedium', 'quicksrnetsmall', 'qwen2_5_7b_instruct', 'qwen2_7b_instruct', 'real_esrgan_general_x4v3', 'real_esrgan_x4plus', 'regnet', 'resnet101', 'resnet18', 'resnet50', 'resnet_2plus1d', 'resnet_3d', 'resnet_mixed', 'resnext101', 'resnext50', 'rtmdet', 'rtmpose_body2d', 'salsanext', 'sam2', 'segformer_base', 'sesr_m5', 'shufflenet_v2', 'simple_bev_cam', 'sinet', 'squeezenet1_1', 'stable_diffusion_v1_5', 'stable_diffusion_v2_1', 'swin_base', 'swin_small', 'swin_tiny', 'trocr', 'unet_segmentation', 'video_mae', 'vit', 'whisper_base_en', 'whisper_large_v3_turbo', 'whisper_medium_en', 'whisper_small_en', 'whisper_small_v2', 'whisper_tiny_en', 'wideresnet50', 'xlsr', 'yamnet', 'yolov10_det', 'yolov11_det', 'yolov11_seg', 'yolov3', 'yolov5', 'yolov6', 'yolov7', 'yolov8_det', 'yolov8_seg', 'yolox', '_shared', '__init__.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "# from qai_hub_models.models.mistral_7b_instruct_v0_3 import Model\n",
    "# from qai_hub_models.models.mistral_7b_instruct_v0_3 import Mistral_7B_Instruct_V0_3\n",
    "import os, sys\n",
    "\n",
    "pkg_path = os.path.join(sys.prefix, \"Lib\", \"site-packages\", \"qai_hub_models\", \"models\")\n",
    "print(os.listdir(pkg_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96c306af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MODEL_ID', 'Model', 'model']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "# Load the model module dynamically\n",
    "mistral_mod = importlib.import_module(\"qai_hub_models.models.mistral_7b_instruct_v0_3\")\n",
    "\n",
    "# Get the class name manually\n",
    "print([x for x in dir(mistral_mod) if not x.startswith(\"_\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d2da26c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Mistral_7B_Instruct_v0_3' has no attribute 'from_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqai_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Device\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Instantiate the model (QNN or default variant)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m      7\u001b[0m response \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssess the threat level of the following actions: punching, running, hugging.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Mistral_7B_Instruct_v0_3' has no attribute 'from_pretrained'"
     ]
    }
   ],
   "source": [
    "from qai_hub_models.models.mistral_7b_instruct_v0_3 import Model\n",
    "from qai_hub import Device\n",
    "# Instantiate the model (QNN or default variant)\n",
    "model = Model.from_pretrained()\n",
    "\n",
    "# Run inference\n",
    "response = model(\"Assess the threat level of the following actions: punching, running, hugging.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e990d945",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2.\n401 Client Error. (Request ID: Root=1-6872bc25-31fb174005927b884d3ad788;e2a22024-b1fc-462a-b253-67455695f74c)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\requests\\models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\transformers\\utils\\hub.py:470\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[1;32m--> 470\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\huggingface_hub\\file_download.py:1008\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\huggingface_hub\\file_download.py:1115\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[1;32m-> 1115\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\huggingface_hub\\file_download.py:1645\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1641\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[0;32m   1642\u001b[0m ):\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m   1644\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1645\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\huggingface_hub\\file_download.py:1533\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1532\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1533\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\huggingface_hub\\file_download.py:1450\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1450\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1459\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\huggingface_hub\\file_download.py:286\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 286\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    287\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    288\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    289\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\huggingface_hub\\file_download.py:310\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    309\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, retry_on_exceptions\u001b[38;5;241m=\u001b[39m(), retry_on_status_codes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m429\u001b[39m,))\n\u001b[1;32m--> 310\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:426\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    423\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m     )\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-6872bc25-31fb174005927b884d3ad788;e2a22024-b1fc-462a-b253-67455695f74c)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral_onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load model and tokenizer\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Define ONNX export config\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:531\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 531\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    532\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    533\u001b[0m     return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    534\u001b[0m     code_revision\u001b[38;5;241m=\u001b[39mcode_revision,\n\u001b[0;32m    535\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    538\u001b[0m )\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1153\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1151\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1153\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1154\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1155\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\transformers\\configuration_utils.py:595\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    594\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 595\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\transformers\\configuration_utils.py:654\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    650\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 654\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\transformers\\utils\\hub.py:312\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[0;32m    255\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    256\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 312\u001b[0m     file \u001b[38;5;241m=\u001b[39m cached_files(path_or_repo_id\u001b[38;5;241m=\u001b[39mpath_or_repo_id, filenames\u001b[38;5;241m=\u001b[39m[filename], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    313\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[1;32mc:\\Users\\qc_de\\anaconda3\\envs\\python-310\\lib\\site-packages\\transformers\\utils\\hub.py:533\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    536\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, LocalEntryNotFoundError):\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n",
      "\u001b[1;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2.\n401 Client Error. (Request ID: Root=1-6872bc25-31fb174005927b884d3ad788;e2a22024-b1fc-462a-b253-67455695f74c)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from optimum.exporters.onnx import main_export\n",
    "from optimum.exporters.onnx.config import OnnxConfigWithPast\n",
    "# from optimum.exporters.onnx.model_export import export\n",
    "from optimum.exporters.onnx.model_configs import TextDecoderOnnxConfig\n",
    "from pathlib import Path\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "output_dir = Path(\"mistral_onnx\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Define ONNX export config\n",
    "onnx_config = TextDecoderOnnxConfig(model.config)\n",
    "\n",
    "# Export model\n",
    "export(\n",
    "    model=model,\n",
    "    config=onnx_config,\n",
    "    opset=13,\n",
    "    output=output_dir,\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Export completed: ONNX model saved to {output_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626e2d2a",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b9565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "from collections import deque\n",
    "import onnx\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d92f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = ort.InferenceSession(\"models/yolov8n.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf27193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: images\n",
      "Shape: [1, 3, 640, 640]\n",
      "Type: tensor(float)\n"
     ]
    }
   ],
   "source": [
    "# Inspect ONNX model input details\n",
    "for input in session.get_inputs():\n",
    "    print(\"Name:\", input.name)\n",
    "    print(\"Shape:\", input.shape)\n",
    "    print(\"Type:\", input.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb70a585",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'qai_hub_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqai_hub_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01multralytics_yolov8\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UltralyticsYOLOv8\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqai_hub_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmediapipe_blazepose\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MediapipeBlazePose\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiate models\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'qai_hub_models'"
     ]
    }
   ],
   "source": [
    "from qai_hub_models.models.ultralytics_yolov8 import UltralyticsYOLOv8\n",
    "from qai_hub_models.models.mediapipe_blazepose import MediapipeBlazePose\n",
    "\n",
    "# Instantiate models\n",
    "yolo = UltralyticsYOLOv8(model_variant=\"nano-int8\")\n",
    "pose_model = MediapipeBlazePose(model_variant=\"lightning-int8\")\n",
    "\n",
    "# Download and prepare for inference\n",
    "yolo.prepare()\n",
    "pose_model.prepare()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Hub_Llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
