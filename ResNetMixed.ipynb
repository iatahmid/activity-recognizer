{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76767e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "from collections import deque\n",
    "import onnx\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917394e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff921652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b56cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abseiling', 'air drumming', 'answering questions', 'applauding', 'applying cream']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt\"\n",
    "lines = urllib.request.urlopen(url).read().decode(\"utf-8\").splitlines()\n",
    "\n",
    "labels = [line.strip() for line in lines]\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0fdd763",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load(\"models/resnet_mixed-resnet-mixed-convolution-float.onnx\")\n",
    "for prop in model.metadata_props:\n",
    "    print(f\"{prop.key}: {prop.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87925d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ONNX model\n",
    "# session = ort.InferenceSession(\"models/resnet_mixed-resnet-mixed-convolution-float.onnx\")\n",
    "# session = ort.InferenceSession(\"models/resnet_2plus1d-resnet-2plus1d-float.onnx\")\n",
    "session = ort.InferenceSession(\"models/video_mae.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f8ee7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: video\n",
      "Shape: [1, 3, 16, 224, 224]\n",
      "Type: tensor(float)\n"
     ]
    }
   ],
   "source": [
    "# Inspect ONNX model input details\n",
    "for input in session.get_inputs():\n",
    "    print(\"Name:\", input.name)\n",
    "    print(\"Shape:\", input.shape)\n",
    "    print(\"Type:\", input.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8fe5142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction buffer: ['cracking neck', 'whistling', 'whistling', 'whistling', 'whistling', 'whistling', 'eating chips', 'answering questions', 'whistling', 'trimming or shaving beard', 'whistling', 'whistling', 'whistling', 'cracking neck', 'cracking neck', 'cracking neck', 'playing harmonica', 'playing harmonica', 'sign language interpreting', 'cracking neck']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess a single frame\n",
    "# def preprocess_frame(frame):\n",
    "#     frame = cv2.resize(frame, (112, 112))\n",
    "#     frame = frame.astype(np.float32) / 255.0\n",
    "#     frame = np.transpose(frame, (2, 0, 1))  # Channels first\n",
    "#     return frame\n",
    "def preprocess_frame(frame):\n",
    "    frame = cv2.resize(frame, (224, 224))\n",
    "    frame = frame.astype(np.float32) / 255.0\n",
    "    frame = np.transpose(frame, (2, 0, 1))  # Channels first: [3, 224, 224]\n",
    "    return frame\n",
    "\n",
    "# Collect 16 frames for each inference\n",
    "frame_buffer = []\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "last_activity = \"\"\n",
    "activity_counter = 0\n",
    "display_duration = 30  # Number of frames to keep the text\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    processed = preprocess_frame(frame)\n",
    "    frame_buffer.append(processed)\n",
    "\n",
    "    if len(frame_buffer) == 16:\n",
    "        video = np.stack(frame_buffer, axis=1)\n",
    "        video = np.expand_dims(video, axis=0)\n",
    "\n",
    "        outputs = session.run(None, {\"video\": video.astype(np.float32)})\n",
    "        prediction = np.argmax(outputs[0])\n",
    "        last_activity = labels[prediction]\n",
    "        activity_counter = display_duration\n",
    "        prediction_buffer.append(last_activity)\n",
    "        frame_buffer = []\n",
    "\n",
    "    # Show text if counter > 0\n",
    "    if activity_counter > 0:\n",
    "        cv2.putText(frame, f\"Activity: {last_activity}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "        activity_counter -= 1\n",
    "\n",
    "    cv2.imshow(\"Activity Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "print(\"Prediction buffer:\", list(prediction_buffer))\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75dc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Hub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
